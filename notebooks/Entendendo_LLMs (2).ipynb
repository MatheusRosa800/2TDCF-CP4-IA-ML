{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# O que é Tokenizer\n",
        "\n",
        "Um tokenizer em Machine Learning é uma ferramenta ou processo que divide o texto em unidades menores chamadas tokens. Esses tokens podem ser palavras, sub-palavras, caracteres ou símbolos, dependendo do tipo de tokenização usada. A tokenização é um passo essencial no processamento de linguagem natural (NLP) e é usada para converter texto bruto em um formato que pode ser compreendido e utilizado por modelos de aprendizado de máquina."
      ],
      "metadata": {
        "id": "zaufoOMyV61e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explorando o Tokenizer"
      ],
      "metadata": {
        "id": "4ETP5RnkVJ8T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificando a presença de uma GPU"
      ],
      "metadata": {
        "id": "ME3kxxfGn_l2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nividia.smi"
      ],
      "metadata": {
        "id": "eMMzUCplhrJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# O que são pipelines?"
      ],
      "metadata": {
        "id": "lV6qi9-6oKEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipelines servem para agrupar as funcões necessarias da tarefa para processar a entrada e gerar a saida."
      ],
      "metadata": {
        "id": "Z8ANTtIdoO7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "gerbo = transformers.pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    device=\"cuda\"\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YvK41TVfejPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo escolhido para treinamento"
      ],
      "metadata": {
        "id": "bWSVEQa1odon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***TinyLlama/TinyLlama-1.1B-Chat-v1.0***\n",
        "\n",
        "O TinyLlama/TinyLlama-1.1B-Chat-v1.0 é um modelo de linguagem treinado para entender e gerar texto de maneira semelhante aos modelos GPT (como o ChatGPT), mas com uma arquitetura e escopo menores, voltado para aplicações específicas ou para rodar em dispositivos com recursos mais limitados.\n",
        "\n",
        "Esse tipo de modelo é útil para desenvolvedores que precisam de uma solução mais compacta e eficiente para tarefas de linguagem natural, especialmente em ambientes onde os modelos grandes seriam impraticáveis devido a limitações de hardware ou custo.\n",
        "\n",
        "[Model Link](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)\n",
        "\n",
        "Caracteristicas\n",
        "\n",
        "| Setting         | Description       |\n",
        "| --------------- | ----------------- |\n",
        "| Content Cell    | 1.1B              |\n",
        "| Layers          | 22                |\n",
        "| Heads           | 32                |\n",
        "| Query Groups    | 4                 |\n",
        "| Embedding Size  |2048               |\n",
        "| Batch Size      | 2 million tokens  |\n",
        "| Total Tokens    | 3 trillion        |\n",
        "| Hardware        | 16 A100-40G GPUs  |\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7EKsqEC7opc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gerbo.model"
      ],
      "metadata": {
        "id": "RMhMron9gXKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Função principal"
      ],
      "metadata": {
        "id": "aQrTdFYWruZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def processa_prompt (modelo,prompt):\n",
        "\n",
        "  # tokenizando e encodando\n",
        "  tokens = modelo.tokenizer.encode(prompt,return_tensors=\"pt\")\n",
        "\n",
        "  print(\"=\"*80)\n",
        "  print(f\"- prompt original: {prompt}\")\n",
        "  print(f\"- tokens:{tokens}\")\n",
        "  print(f\"- # de tokens:{len(tokens)}\")\n",
        "\n",
        "  # fazendo o processo reverso --- decodificando\n",
        "\n",
        "  decoded = modelo.tokenizer.convert_ids_to_tokens(tokens[0])\n",
        "  print(f\"- tokens decodificados: {decoded}\")\n",
        "\n",
        "  first_tokens = modelo.tokenizer.convert_ids_to_tokens([0,1,2,3,4,5])\n",
        "  print(f\"- primeiros tokens: {first_tokens}\")\n",
        "\n",
        "  # É preciso setar no sample para controlar a temperatura\n",
        "  # Quanto maior a temperatura, mais criativo o modelo será.\n",
        "  output = modelo(prompt, do_sample=True, temperature=0.7)\n",
        "\n",
        "  print(\"=\"*80)\n",
        "  print(f\"- output: {output}\")\n",
        "  print(f\"- texto gerado efetivamente: {output[0]['generated_text']}\")\n",
        "\n",
        "  print(\"=\"*80)\n",
        "  return\n",
        "\n",
        "processa_prompt(gerbo,\"Once upon a time\")"
      ],
      "metadata": {
        "id": "97laIybgiO38"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}